{"cells":[{"cell_type":"markdown","metadata":{"id":"BKufZ65aAE0G"},"source":["# Uploading the dataset\n","\n","##First upload the dataset manually using the \"Files\" section in the sidebar. The names of the test and train files should be \"text.txt\" and \"train.txt\". This block reads the files and create csv files according to uploaded files.\n","\n","#### *Note: If you want to run this block again first remove train and test csv files.\n","\n","#### *Note: You should do these steps for each runtime as google colab recycles allocated resources every 12 hours####\n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"ny2gXCzXvcWU"},"source":["# Intalling required dependancies, Importing required libraries and nltk modules"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"-SzgDOY2rPmI","executionInfo":{"status":"ok","timestamp":1750957744973,"user_tz":-210,"elapsed":30805,"user":{"displayName":"Ali Yazdanpanah","userId":"06011185636208919827"}},"outputId":"549aa0c3-d331-4cf5-a8ad-a60cae3e0261"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (7.1.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open) (1.17.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"127a3e2b3a6b47d4bc4dd3ae93dbb537"}},"metadata":{}}],"source":["!pip install gensim smart_open scikit-learn nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":6108,"status":"error","timestamp":1750956252706,"user":{"displayName":"Ali Yazdanpanah","userId":"06011185636208919827"},"user_tz":-210},"id":"B6L1QgKKoI5L","outputId":"90d486dc-b211-4dae-8cd0-c651c33a8be0"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'gensim'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-2905232473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import glob\n","import json\n","import os\n","import numpy as np\n","import nltk\n","import sklearn\n","import collections\n","import smart_open\n","import random\n","import gensim\n","import time\n","import difflib\n","import string\n","\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from collections import defaultdict\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet as wn\n","from gensim.models import KeyedVectors\n","from sklearn.svm import SVR, LinearSVR\n","from sklearn.linear_model import LinearRegression\n","\n","\n","nltk.download('wordnet_ic')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n"]},{"cell_type":"markdown","metadata":{"id":"wEK2r77jvxjV"},"source":["#Removing index in file and adding it to `new_test.txt` and `new_train.txt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzewPBFR_RdR"},"outputs":[],"source":["for filename in glob.glob('*.txt'):\n","  with open(filename, 'r') as f:\n","    lines = list(f.readlines())\n","    name = filename.split(\".txt\")[0]\n","    if \"test\" == name or \"train\" == name:\n","      with open(\"/content/new_\" + name + \".txt\", \"a+\") as file:\n","        for line in lines[1:]:\n","          splitted = line.split(\"\\t\")\n","          file.write(splitted[1] + \"\\t\" + splitted[2] + \"\\t\" + splitted[3])"]},{"cell_type":"markdown","metadata":{"id":"27L_4MiCv6hL"},"source":["# Downloading Google News corpus (3 billion running words) word vector model (3 million 300-dimension English word vectors)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1750956225852,"user":{"displayName":"Ali Yazdanpanah","userId":"06011185636208919827"},"user_tz":-210},"id":"Zioc6MIvd33S","outputId":"4f0e8139-b4ea-4fba-b138-f10e864c6c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-26 16:49:48--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.32.158, 52.217.96.14, 52.216.222.80, ...\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.32.158|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2025-06-26 16:49:48 ERROR 404: Not Found.\n","\n","gzip: /content/GoogleNews-vectors-negative300.bin.gz: No such file or directory\n"]}],"source":["!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"]},{"cell_type":"markdown","metadata":{"id":"b4eNVattwaXW"},"source":["#Initializing word vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nh5YFRK0gdQS"},"outputs":[],"source":["vec1 = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin\", binary=True)"]},{"cell_type":"markdown","metadata":{"id":"MOlAXsOXwl8y"},"source":["#Extract sentence similarity based on vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrcD_pxje0Ke"},"outputs":[],"source":["def extract_res_vec_similarity(s1, s2):\n","    first_sents_embeddings = np.empty([0,300])\n","    second_sents_embeddings = np.empty([0,300])\n","\n","    first_vecs = np.array([])\n","    for w in s1.split():\n","        w = w.strip('?.,')\n","        if w in vec1:\n","            first_vec = np.array([vec1[w]])\n","            if first_vecs.shape[0] == 0:\n","                first_vecs = first_vec\n","            else:\n","                first_vecs = np.vstack((first_vecs, first_vec))\n","        else:\n","            if first_vecs.shape[0] == 0:\n","                first_vecs = np.random.normal(0, 5, 300)\n","            else:\n","                first_vecs = np.vstack((first_vecs, np.random.normal(0, 5, 300)))\n","        # print(\"first \")\n","        # print(first_vecs.shape)\n","    if(first_vecs.shape == (300, )):\n","        temp = first_vecs\n","    else:\n","        temp = np.mean(first_vecs, axis=0)\n","    # print(temp.shape)\n","    first_sents_embeddings = np.append(first_sents_embeddings, [temp], axis=0)\n","\n","    second_vecs = np.array([])\n","    for w in s2.split():\n","        w = w.strip('?.,')\n","        if w in vec1:\n","            second_vec = np.array([vec1[w]])\n","            if second_vecs.shape[0] == 0:\n","                second_vecs = second_vec\n","            else:\n","                second_vecs = np.vstack((second_vecs, second_vec))\n","        else:\n","            if second_vecs.shape[0] == 0:\n","                second_vecs = np.random.normal(0, 5, 300)\n","            else:\n","                second_vecs = np.vstack((second_vecs, np.random.normal(0, 5, 300)))\n","        # print(\"second \")\n","        # print(second_vecs.shape)\n","    if(second_vecs.shape == (300,)):\n","        temp = second_vecs\n","    else:\n","        temp = np.mean(second_vecs, axis=0)\n","    # print(temp.shape)\n","    second_sents_embeddings = np.append(second_sents_embeddings, [temp], axis=0)\n","\n","    for i in range(len(first_sents_embeddings)):\n","        # cosine similarity\n","\n","        ret = np.dot(first_sents_embeddings[i], second_sents_embeddings[i]) / (np.linalg.norm(first_sents_embeddings[i]) * np.linalg.norm(second_sents_embeddings[i]))\n","        ret = 5*(ret + 1) / 2\n","\n","    return ret"]},{"cell_type":"markdown","metadata":{"id":"akQJ66jhxJg_"},"source":[]},{"cell_type":"markdown","metadata":{"id":"x8SjiZq6wvkC"},"source":["#InferSent\n","##*Note: This section uses InferSent to calculate sentence similiraty score based on it's pre-trained models which are not available for public access at the moment you can upload infersent1 or infersnet2 locally, if you don't have access to these files skip to the next section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXYCHEIVoRYh"},"outputs":[],"source":["!curl -Lo infersent2.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent2.pkl\n","!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n","!unzip glove.840B.300d.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEzd3VxMZ7wP"},"outputs":[],"source":["# Copyright (c) 2017-present, Facebook, Inc.\n","# All rights reserved.\n","#\n","# This source code is licensed under the license found in the\n","# LICENSE file in the root directory of this source tree.\n","#\n","\n","\"\"\"\n","This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf\n","\"\"\"\n","\n","import numpy as np\n","import time\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class InferSent(nn.Module):\n","\n","    def __init__(self, config):\n","        super(InferSent, self).__init__()\n","        self.bsize = config['bsize']\n","        self.word_emb_dim = config['word_emb_dim']\n","        self.enc_lstm_dim = config['enc_lstm_dim']\n","        self.pool_type = config['pool_type']\n","        self.dpout_model = config['dpout_model']\n","        self.version = 1 if 'version' not in config else config['version']\n","\n","        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n","                                bidirectional=True, dropout=self.dpout_model)\n","\n","        assert self.version in [1, 2]\n","        if self.version == 1:\n","            self.bos = '<s>'\n","            self.eos = '</s>'\n","            self.max_pad = True\n","            self.moses_tok = False\n","        elif self.version == 2:\n","            self.bos = '<p>'\n","            self.eos = '</p>'\n","            self.max_pad = False\n","            self.moses_tok = True\n","\n","    def is_cuda(self):\n","        # either all weights are on cpu or they are on gpu\n","        return self.enc_lstm.bias_hh_l0.data.is_cuda\n","\n","    def forward(self, sent_tuple):\n","        # sent_len: [max_len, ..., min_len] (bsize)\n","        # sent: (seqlen x bsize x worddim)\n","        sent, sent_len = sent_tuple\n","\n","        # Sort by length (keep idx)\n","        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n","        sent_len_sorted = sent_len_sorted.copy()\n","        idx_unsort = np.argsort(idx_sort)\n","\n","        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n","            else torch.from_numpy(idx_sort)\n","        sent = sent.index_select(1, idx_sort)\n","\n","        # Handling padding in Recurrent Networks\n","        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n","        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n","        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n","\n","        # Un-sort by length\n","        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n","            else torch.from_numpy(idx_unsort)\n","        sent_output = sent_output.index_select(1, idx_unsort)\n","\n","        # Pooling\n","        if self.pool_type == \"mean\":\n","            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n","            emb = torch.sum(sent_output, 0).squeeze(0)\n","            emb = emb / sent_len.expand_as(emb)\n","        elif self.pool_type == \"max\":\n","            if not self.max_pad:\n","                sent_output[sent_output == 0] = -1e9\n","            emb = torch.max(sent_output, 0)[0]\n","            if emb.ndimension() == 3:\n","                emb = emb.squeeze(0)\n","                assert emb.ndimension() == 2\n","\n","        return emb\n","\n","    def set_w2v_path(self, w2v_path):\n","        self.w2v_path = w2v_path\n","\n","    def get_word_dict(self, sentences, tokenize=True):\n","        # create vocab of words\n","        word_dict = {}\n","        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n","        for sent in sentences:\n","            for word in sent:\n","                if word not in word_dict:\n","                    word_dict[word] = ''\n","        word_dict[self.bos] = ''\n","        word_dict[self.eos] = ''\n","        return word_dict\n","\n","    def get_w2v(self, word_dict):\n","        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n","        # create word_vec with w2v vectors\n","        word_vec = {}\n","        with open(self.w2v_path, encoding='utf-8') as f:\n","            for line in f:\n","                word, vec = line.split(' ', 1)\n","                if word in word_dict:\n","                    word_vec[word] = np.fromstring(vec, sep=' ')\n","        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n","        return word_vec\n","\n","    def get_w2v_k(self, K):\n","        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n","        # create word_vec with k first w2v vectors\n","        k = 0\n","        word_vec = {}\n","        with open(self.w2v_path, encoding='utf-8') as f:\n","            for line in f:\n","                word, vec = line.split(' ', 1)\n","                if k <= K:\n","                    word_vec[word] = np.fromstring(vec, sep=' ')\n","                    k += 1\n","                if k > K:\n","                    if word in [self.bos, self.eos]:\n","                        word_vec[word] = np.fromstring(vec, sep=' ')\n","\n","                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n","                    break\n","        return word_vec\n","\n","    def build_vocab(self, sentences, tokenize=True):\n","        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n","        word_dict = self.get_word_dict(sentences, tokenize)\n","        self.word_vec = self.get_w2v(word_dict)\n","        print('Vocab size : %s' % (len(self.word_vec)))\n","\n","    # build w2v vocab with k most frequent words\n","    def build_vocab_k_words(self, K):\n","        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n","        self.word_vec = self.get_w2v_k(K)\n","        print('Vocab size : %s' % (K))\n","\n","    def update_vocab(self, sentences, tokenize=True):\n","        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n","        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n","        word_dict = self.get_word_dict(sentences, tokenize)\n","\n","        # keep only new words\n","        for word in self.word_vec:\n","            if word in word_dict:\n","                del word_dict[word]\n","\n","        # udpate vocabulary\n","        if word_dict:\n","            new_word_vec = self.get_w2v(word_dict)\n","            self.word_vec.update(new_word_vec)\n","        else:\n","            new_word_vec = []\n","        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n","\n","    def get_batch(self, batch):\n","        # sent in batch in decreasing order of lengths\n","        # batch: (bsize, max_len, word_dim)\n","        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n","\n","        for i in range(len(batch)):\n","            for j in range(len(batch[i])):\n","                embed[j, i, :] = self.word_vec[batch[i][j]]\n","\n","        return torch.FloatTensor(embed)\n","\n","    def tokenize(self, s):\n","        from nltk.tokenize import word_tokenize\n","        if self.moses_tok:\n","            s = ' '.join(word_tokenize(s))\n","            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n","            return s.split()\n","        else:\n","            return word_tokenize(s)\n","\n","    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n","        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n","                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n","        n_w = np.sum([len(x) for x in sentences])\n","\n","        # filters words without w2v vectors\n","        for i in range(len(sentences)):\n","            s_f = [word for word in sentences[i] if word in self.word_vec]\n","            if not s_f:\n","                import warnings\n","                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n","                               Replacing by \"</s>\"..' % (sentences[i], i))\n","                s_f = [self.eos]\n","            sentences[i] = s_f\n","\n","        lengths = np.array([len(s) for s in sentences])\n","        n_wk = np.sum(lengths)\n","        if verbose:\n","            print('Nb words kept : %s/%s (%.1f%s)' % (\n","                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n","\n","        # sort by decreasing length\n","        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n","        sentences = np.array(sentences)[idx_sort]\n","\n","        return sentences, lengths, idx_sort\n","\n","    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n","        tic = time.time()\n","        sentences, lengths, idx_sort = self.prepare_samples(\n","                        sentences, bsize, tokenize, verbose)\n","\n","        embeddings = []\n","        for stidx in range(0, len(sentences), bsize):\n","            batch = self.get_batch(sentences[stidx:stidx + bsize])\n","            if self.is_cuda():\n","                batch = batch.cuda()\n","            with torch.no_grad():\n","                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n","            embeddings.append(batch)\n","        embeddings = np.vstack(embeddings)\n","\n","        # unsort\n","        idx_unsort = np.argsort(idx_sort)\n","        embeddings = embeddings[idx_unsort]\n","\n","        if verbose:\n","            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n","                    len(embeddings)/(time.time()-tic),\n","                    'gpu' if self.is_cuda() else 'cpu', bsize))\n","        return embeddings\n","\n","    def visualize(self, sent, tokenize=True):\n","\n","        sent = sent.split() if not tokenize else self.tokenize(sent)\n","        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n","\n","        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n","            import warnings\n","            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n","                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n","        batch = self.get_batch(sent)\n","\n","        if self.is_cuda():\n","            batch = batch.cuda()\n","        output = self.enc_lstm(batch)[0]\n","        output, idxs = torch.max(output, 0)\n","        # output, idxs = output.squeeze(), idxs.squeeze()\n","        idxs = idxs.data.cpu().numpy()\n","        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n","\n","        # visualize model\n","        import matplotlib.pyplot as plt\n","        x = range(len(sent[0]))\n","        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n","        plt.xticks(x, sent[0], rotation=45)\n","        plt.bar(x, y)\n","        plt.ylabel('%')\n","        plt.title('Visualisation of words importance')\n","        plt.show()\n","\n","        return output, idxs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEJ77UnC-nZM"},"outputs":[],"source":["from nltk.corpus import wordnet_ic\n","from gensim.models import KeyedVectors\n","import torch\n","import nltk\n","nltk.download('punkt')\n","\n","V = 2\n","MODEL_PATH = '/content/infersent%s.pkl' % V\n","params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n","                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n","infersent = InferSent(params_model)\n","infersent.load_state_dict(torch.load(MODEL_PATH))\n","\n","brown_ic = wordnet_ic.ic('ic-brown.dat')\n","nltk.download('punkt')\n","infersent.set_glove_path('/content/glove.840B.300d.txt')\n","infersent.build_vocab_k_words(K=100000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgFGJ5eSqxC7"},"outputs":[],"source":["\n","def extract_sent2vec_similarity(s1, s2):\n","    eb1 = infersent.encode([s1], tokenize=True)\n","    eb2 = infersent.encode([s2], tokenize=True)\n","    a = np.squeeze(eb1).tolist()\n","    b = np.squeeze(eb2).tolist()\n","    ret = 1 - spatial.distance.cosine(a,b)\n","    return ret"]},{"cell_type":"markdown","metadata":{"id":"JMROpDldxk02"},"source":["#Simple baseline\n","##For the simple baseline an unsupervised approach by creating sentence vectors with each dimension representing whether an individual word appears in a sentence is used. The final score is calculated using cosine similairty between the sentence vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eqCm-bmLPNE"},"outputs":[],"source":["def sentence_similarity_simple_baseline(s1, s2,counts = None):\n","    def embedding_count(s):\n","        ret_embedding = defaultdict(int)\n","        for w in s.split():\n","            w = w.strip('?.,')\n","            ret_embedding[w] += 1\n","        return ret_embedding\n","    first_sent_embedding = embedding_count(s1)\n","    second_sent_embedding = embedding_count(s2)\n","    Embedding1 = []\n","    Embedding2 = []\n","    if counts:\n","        for w in first_sent_embedding:\n","            Embedding1.append(first_sent_embedding[w] * 1.0/ (counts[w]+0.001))\n","            Embedding2.append(second_sent_embedding[w] *1.0/ (counts[w]+0.001))\n","    else:\n","        for w in first_sent_embedding:\n","            Embedding1.append(first_sent_embedding[w])\n","            Embedding2.append(second_sent_embedding[w])\n","    ret_score = 0\n","    if not 0 == sum(Embedding2):\n","        sm= difflib.SequenceMatcher(None,Embedding1,Embedding2)\n","        ret_score = sm.ratio()*5\n","    return ret_score"]},{"cell_type":"markdown","metadata":{"id":"sdz8rLrdyG86"},"source":["#Word alignment\n","##Look for each word of sentence A in Wordnet, we can get a synonym set (synset). Then align this word with a corresponding one in sentence B by using path similarity in Wordnet trees. The labeled alignments are used as one useful feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bc_ztOClLQF-"},"outputs":[],"source":["# =========== util func ==============\n","def penn_to_wn(tag):\n","    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n","    if tag.startswith('N'): return 'n'\n","    if tag.startswith('V'): return 'v'\n","    if tag.startswith('J'): return 'a'\n","    if tag.startswith('R'): return 'r'\n","    return None\n","\n","def tagged_to_synset(word, tag):\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag is None:\n","        return None\n","    try:\n","        return wn.synsets(word, wn_tag)[0]\n","    except:\n","        return None\n","\n","# =========== feature extraction ==============\n","def sentence_similarity_word_alignment(sentence1, sentence2):\n","    \"\"\" compute the sentence similarity using Wordnet and ppdb \"\"\"\n","    # Tokenize and tag\n","    sentence1 = pos_tag(word_tokenize(sentence1))\n","    sentence2 = pos_tag(word_tokenize(sentence2))\n","    # Get the synsets for the tagged words\n","    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n","    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n","    # Filter out the Nones\n","    synsets1 = [ss for ss in synsets1 if ss]\n","    synsets2 = [ss for ss in synsets2 if ss]\n","    score, count = 0.0, 0\n","    ppdb_score, align_cnt = 0, 0\n","    # For each word in the first sentence\n","    for synset in synsets1:\n","        # Get the similarity value of the most similar word in the other sentence\n","        L = [synset.path_similarity(ss) for ss in synsets2]\n","        L_prime = L\n","        L = [l for l in L if l]\n","        if L:\n","            best_score = max(L)\n","            score += best_score\n","            count += 1\n","    # Average the values\n","    if count >0: score /= count\n","\n","    # ppdb_wa_pen_ua features\n","    # len_s1, len_s2 = len(sentence1), len(sentence2)\n","    # ppdb_score = (1 - 0.4 * (len_s1 + len_s2 - 2*align_cnt) / float(len_s1 + len_s2)) * ppdb_score\n","    return score#, ppdb_score\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wgiqHunkyR2_"},"source":["#Unigram Overlap\n","##Count the unigram overlap between sentence A and sentence B, with synonym check. Then Calculate the ration of overlap count over the total length of two sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7uVG8t9L8bJ"},"outputs":[],"source":["def extract_overlap_pen(s1, s2):\n","    \"\"\"\n","    :param s1:\n","    :param s2:\n","    :return: overlap_pen score\n","    \"\"\"\n","    ss1 = s1.strip().split()\n","    ss2 = s2.strip().split()\n","    ovlp_cnt = 0\n","    for w1 in ss1:\n","        ovlp_cnt += ss2.count(w1)\n","    score = 2 * ovlp_cnt / (len(ss1) + len(ss2) + .0)\n","    return score\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bJGWYLkwyYGz"},"source":["#Absolute Difference\n","##Let Cta and Cta be the counts of tokens of type t ∈ {all tokens, adjectives, adverbs, nouns, and verbs} in sentence A and B respectively. Then calculate the absolute difference as |Cta−Ctb|/(Cta+Ctb)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buAZwBY6MB9P"},"outputs":[],"source":["def extract_absolute_difference(s1, s2):\n","    \"\"\"t \\in {all tokens, adjectives, adverbs, nouns, and verbs}\"\"\"\n","    s1, s2 = word_tokenize(s1), word_tokenize(s2)\n","    pos1, pos2 = pos_tag(s1), pos_tag(s2)\n","    # all tokens\n","    t1 = abs(len(s1) - len(s2)) / float(len(s1) + len(s2))\n","    # all adjectives\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('J')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('J')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t2 = 0\n","    else:\n","        t2 = abs(cnt1 - cnt2) / float(cnt1 + cnt2)\n","    # all adverbs\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('R')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('R')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t3 = 0\n","    else:\n","        t3 = abs(cnt1 - cnt2) / float(cnt1 + cnt2)\n","    # all nouns\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('N')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('N')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t4 = 0\n","    else:\n","        t4 = abs(cnt1 - cnt2) / float(cnt1 + cnt2)\n","    # all verbs\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('V')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('V')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t5 = 0\n","    else:\n","        t5 = abs(cnt1 - cnt2) / float(cnt1 + cnt2)\n","    return [t1, t2, t3, t4, t5]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nnfYZs7iymZk"},"source":["#Min to Max Ratio\n","##Let Ct1 and Ct2 be the counts of type t ∈ {all, adjectives, adverbs, nouns, and verbs} for shorter Sentence 1 and longer Sentence 2 respectively. then calculate the minimum to maximum ratio Ct1/Ct2 as one feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flDpOesyME1A"},"outputs":[],"source":["def extract_mmr_t(s1, s2):\n","    shorter = 1\n","    if(len(s1) > len(s2)):  shorter = 2\n","\n","    s1, s2 = word_tokenize(s1), word_tokenize(s2)\n","    pos1, pos2 = pos_tag(s1), pos_tag(s2)\n","    # all tokens\n","    t1 = (len(s1)+0.001) / (len(s2) +0.001)\n","    # all adjectives\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('J')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('J')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t2 = 0\n","    else:\n","        t2 = (cnt1 +0.001) / (cnt2 + 0.001)\n","    # all adverbs\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('R')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('R')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t3 = 0\n","    else:\n","        t3 = (cnt1 +0.001) / (cnt2+0.001)\n","    # all nouns\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('N')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('N')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t4 = 0\n","    else:\n","        t4 = (cnt1 +0.001) / (cnt2 +0.001)\n","    # all verbs\n","    cnt1 = len([1 for item in pos1 if item[1].startswith('V')])\n","    cnt2 = len([1 for item in pos2 if item[1].startswith('V')])\n","    if cnt1 == 0 and cnt2 == 0:\n","        t5 = 0\n","    else:\n","        t5 = (cnt1+ 0.001) / (cnt2 + 0.001)\n","\n","    if shorter == 2:\n","        t1 = 1 / (t1 + 0.001)\n","        t2 = 1 / (t2 + 0.001)\n","        t3 = 1 / (t3 + 0.001)\n","        t4 = 1 / (t4 + 0.001)\n","        t5 = 1 / (t5 + 0.001)\n","\n","    return [t1, t2, t3, t4, t5]\n"]},{"cell_type":"markdown","metadata":{"id":"WkcKcQn-zC46"},"source":["\n","#Resnik Similarity using Information Content from Brown Corpus\n","\n","##use information content generated from the Brown corpus to compute the resnik similarity between paths in the wordnet trees for the given sentence pairs. This approach uses IC of the Least Common Subsumer (most specific ancestor node) to output a score which is used by the Support Vector Regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWItt6nAMIDm"},"outputs":[],"source":["def penn_to_wn(tag):\n","    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n","    if tag.startswith('N'): return 'n'\n","    if tag.startswith('V'): return 'v'\n","    if tag.startswith('J'): return 'a'\n","    if tag.startswith('R'): return 'r'\n","    return None\n","\n","def tagged_to_synset(word, tag):\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag is None:\n","        return None\n","    try:\n","        return wn.synsets(word, wn_tag)[0]\n","    except:\n","        return None\n","\n","\n","def sentence_similarity_information_content(sentence1, sentence2):\n","\n","    ''' compute the sentence similairty using information content from wordnet '''\n","    # Tokenize and tag\n","    sentence1 = pos_tag(word_tokenize(sentence1))\n","    sentence2 = pos_tag(word_tokenize(sentence2))\n","    # Get the synsets for the tagged words\n","    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n","    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n","    # Filter out the Nones\n","    synsets1 = [ss for ss in synsets1 if ss]\n","    synsets2 = [ss for ss in synsets2 if ss]\n","    score, count = 0.0, 0\n","    ppdb_score, align_cnt = 0, 0\n","    # For each word in the first sentence\n","    for synset in synsets1:\n","        L = []\n","        for ss in synsets2:\n","            try:\n","                L.append(synset.res_similarity(ss, brown_ic))\n","            except:\n","                continue\n","        if L:\n","            best_score = max(L)\n","            score += best_score\n","            count += 1\n","    # Average the values\n","    if count >0: score /= count\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"b0UhfPGczMNi"},"source":["#Running\n","\n","##In this section a similarty score for each pair in the datasets is calculated using all above methods (except InferSent, for more info see InferSent section), Then the scores are feed to a SVR model as features. Results for the test set are stored in `results.txt`\n","\n","#Preprocessing\n","##Used tokenization and lemmatization on the data as a pre-processing step before we turn the data into our models. Chose to do this step as lemmatization does not take away any semantic information from sentences and hence was an essential step for our application."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60333,"status":"ok","timestamp":1611098145375,"user":{"displayName":"Ali Yazdanpanah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDbsKF0PQiIRjS4uxadXKEltJw0kPMiKWxSIZC=s64","userId":"06011185636208919827"},"user_tz":-210},"id":"MovwWeD8MZFs","outputId":"13a2ec00-4839-4c90-bf3c-82efd82d2e01"},"outputs":[{"name":"stdout","output_type":"stream","text":["9.99 %finished ( 3.9240200519561768 )\n","19.98 %finished ( 7.12114143371582 )\n","29.97 %finished ( 10.031977891921997 )\n","39.97 %finished ( 13.225254535675049 )\n","49.96 %finished ( 17.69743537902832 )\n","59.95 %finished ( 22.069612503051758 )\n","69.94 %finished ( 25.401242971420288 )\n","79.93 %finished ( 29.57795023918152 )\n","89.92 %finished ( 34.00630068778992 )\n","99.92 %finished ( 38.37580108642578 )\n","Elapsed time: 40.243180990219116 (preprocessing)\n","10.00 %finished ( 1.296189546585083 )\n","19.99 %finished ( 2.7547202110290527 )\n","29.99 %finished ( 3.9554872512817383 )\n","39.98 %finished ( 4.9682416915893555 )\n","49.98 %finished ( 6.041013717651367 )\n","59.97 %finished ( 7.044367074966431 )\n","69.97 %finished ( 8.072548151016235 )\n","79.97 %finished ( 9.045163869857788 )\n","89.96 %finished ( 10.005644798278809 )\n","99.96 %finished ( 11.002928733825684 )\n","Elapsed time: 58.750736951828\n"]}],"source":["\n","T0 =time.time()\n","#----------\n","# training\n","first_sents = []\n","second_sents = []\n","true_score = []\n","\n","\n","def lower(string):\n","  return str.lower(string)\n","\n","def normalize(corpus):\n","  lemmatizer = WordNetLemmatizer()\n","  stop_words = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"knows\",\"known\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n","  normalized_corpus = []\n","  for s in corpus:\n","    normalized_s = []\n","    for w in s:\n","      if w.isdecimal() or w.isdigit():\n","        normalized_s.append('NUM')\n","      elif w in string.punctuation:\n","        pass\n","      elif w in stop_words:\n","        pass\n","      else:\n","        normalized_s.append(lemmatizer.lemmatize(w.lower()))\n","    normalized_corpus.append(normalized_s)\n","  return normalized_corpus\n","\n","\n","tokenizer = TweetTokenizer()\n","\n","\n","with open(\"new_train.txt\",'r') as f:\n","    for line in f.readlines():\n","        line_split = line.split('\\t')\n","        first_sentence = line_split[0]\n","        second_sentence = line_split[1]\n","        gs = line_split[2]\n","        first_sents.append(first_sentence)\n","        second_sents.append(second_sentence)\n","        true_score.append(gs)\n","\n","word_tokenized = [tokenizer.tokenize(s) for s in first_sents]\n","first_sents_temp = normalize(word_tokenized)\n","\n","first_sentence = []\n","for s in first_sents_temp:\n","  temp = \"\"\n","  for w in s:\n","    temp += w + \" \"\n","  first_sentence += [temp]\n","\n","word_tokenized = [tokenizer.tokenize(s) for s in second_sents]\n","second_sents_temp = normalize(word_tokenized)\n","\n","second_sents = []\n","for s in second_sents_temp:\n","  temp = \"\"\n","  for w in s:\n","    temp += w + \" \"\n","  second_sents += [temp]\n","\n","\n","Counts_for_tf = defaultdict(int)\n","\n","for sent in first_sents:\n","    for w in [w.strip(\"?.,\") for w in sent.split()]: Counts_for_tf[w] += 1\n","for sent in second_sents:\n","    for w in [w.strip(\"?.,\") for w in sent.split()]: Counts_for_tf[w] += 1\n","\n","feature_scores = []\n","N = len(first_sents)\n","T1 = time.time()\n","for i in range(N):\n","    s1 = first_sents[i]\n","    s2 = second_sents[i]\n","\n","\n","    scores = [ sentence_similarity_simple_baseline(s1,s2, Counts_for_tf),\n","                sentence_similarity_word_alignment(s1,s2)\n","                ,sentence_similarity_information_content(s1,s2)\n","                , extract_overlap_pen(s1, s2)\n","                ,*extract_absolute_difference(s1, s2)\n","                ,*extract_mmr_t(s1, s2)\n","              ]\n","    # cosine similarity\n","    feature_scores.append(scores)\n","    if 0 == (i+1) % int(N/10): print(\"%.2f\" % ( (i +1)*1.0/ N *100), \"%\"+\"finished (\",time.time() - T1,\")\")\n","\n","scaler = sklearn.preprocessing.StandardScaler(); scaler.fit(feature_scores); X_features = scaler.transform(feature_scores)\n","print(\"Elapsed time:\",time.time() - T0,\"(preprocessing)\")\n","clf = SVR(kernel='linear')\n","clf.fit(X_features, true_score)\n","\n","#-----------\n","# predicting\n","first_sents = []\n","second_sents = []\n","with open(\"new_test.txt\",'r') as f_val:\n","    for line in f_val.readlines():\n","        line_split = line.split('\\t')\n","        first_sentence = line_split[0]\n","        second_sentence = line_split[1]\n","        first_sents.append(first_sentence)\n","        second_sents.append(second_sentence)\n","\n","word_tokenized = [tokenizer.tokenize(s) for s in first_sents]\n","first_sents_temp = normalize(word_tokenized)\n","\n","first_sentence = []\n","for s in first_sents_temp:\n","  temp = \"\"\n","  for w in s:\n","    temp += w + \" \"\n","  first_sentence += [temp]\n","\n","word_tokenized = [tokenizer.tokenize(s) for s in second_sents]\n","second_sents_temp = normalize(word_tokenized)\n","\n","second_sents = []\n","for s in second_sents_temp:\n","  temp = \"\"\n","  for w in s:\n","    temp += w + \" \"\n","  second_sents += [temp]\n","\n","for sent in first_sents:\n","    for w in [w.strip(\"?.,\") for w in sent.split()]: Counts_for_tf[w] += 1\n","for sent in second_sents:\n","    for w in [w.strip(\"?.,\") for w in sent.split()]: Counts_for_tf[w] += 1\n","\n","\n","feature_scores = []\n","N = len(first_sents)\n","T1 = time.time()\n","for i in range(N):\n","    s1 = first_sents[i]\n","    s2 = second_sents[i]\n","\n","    scores = [ sentence_similarity_simple_baseline(s1,s2, Counts_for_tf),\n","                sentence_similarity_word_alignment(s1,s2)\n","                ,sentence_similarity_information_content(s1,s2)\n","                ,extract_overlap_pen(s1, s2)\n","                ,*extract_absolute_difference(s1, s2)\n","                ,*extract_mmr_t(s1, s2)\n","                ]\n","\n","    feature_scores.append(scores)\n","    if 0 == (i+1) % int(N/10): print(\"%.2f\" % ((i+1) *1.0 / N *100),\"%\"+\"finished (\",time.time() - T1,\")\")\n","X_features = scaler.transform(feature_scores)\n","Y_pred_np = clf.predict(X_features)\n","Y_pred_np = [min(5,max(0,p),p) for p in Y_pred_np]\n","with open(\"results.txt\",'w+') as f_pred:\n","    for i in range(len(Y_pred_np)):\n","        f_pred.write(str(Y_pred_np[i])+'\\n')\n","print(\"Elapsed time:\",time.time() - T0)"]},{"cell_type":"markdown","metadata":{"id":"_ekHBVPB0G-O"},"source":["#Evaluation\n","\n","##Using human assigned scores and predicted scores from test dataset, we then calculate pearson score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LciawciysG4P"},"outputs":[],"source":["import pprint\n","import argparse\n","import math\n","import numpy as np\n","from scipy.stats import pearsonr\n","\n","\n","\n","def main():\n","    with open(\"new_test.txt\",'r') as f1:\n","        with open(\"results.txt\",'r') as f2:\n","            gold = f1.readlines()\n","            pred = f2.readlines()\n","            # http://alt.qcri.org/semeval2017/task1/\n","            # evaluation -- Pearson correlation\n","            gold = [float(g.split()[-1]) for g in gold]\n","            pred = [float(p.split()[-1]) for p in pred]\n","    # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html\n","    ret = pearsonr(gold,pred)[0]\n","\n","    print(\"Performance:\",ret)\n","\n","main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyONwBBlXbCjJK+lGgD+uKuh"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}